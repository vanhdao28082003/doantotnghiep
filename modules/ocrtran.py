import cv2
import numpy as np
import torch
import torch.backends.cudnn as cudnn
from torch.autograd import Variable
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from PIL import Image
import matplotlib.pyplot as plt

import sys
import os

# Th∆∞ m·ª•c ch·ª©a file ocrtran.py (modules/)
MODULE_DIR = os.path.dirname(os.path.abspath(__file__))

# Th∆∞ m·ª•c cha: LUANVANTOTNGHIEP/
PROJECT_DIR = os.path.dirname(MODULE_DIR)

# ƒê∆∞·ªùng d·∫´n t·ªõi CRAFT-pytorch (relative, kh√¥ng c·∫ßn ·ªï ƒëƒ©a)
craft_path = os.path.join(PROJECT_DIR, "CRAFT-pytorch")

# Th√™m v√†o sys.path
sys.path.insert(0, craft_path)



# CRAFT model imports v√† utilities
class CRAFT():
    def __init__(self):
        from craft import CRAFT as CRAFTModel
        from craft_utils import getDetBoxes, adjustResultCoordinates
        from imgproc import resize_aspect_ratio, normalizeMeanVariance
        import craft_utils
        import imgproc
        
        self.CRAFTModel = CRAFTModel
        self.getDetBoxes = getDetBoxes
        self.adjustResultCoordinates = adjustResultCoordinates
        self.resize_aspect_ratio = resize_aspect_ratio
        self.normalizeMeanVariance = normalizeMeanVariance
        self.craft_utils = craft_utils
        self.imgproc = imgproc
        
        # Load model CRAFT
        self.net = CRAFTModel()
        model_path = os.path.join(craft_path, 'craft_mlt_25k.pth')
        if not os.path.exists(model_path):
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file model: {model_path}")
        # T·∫£i model t·ª± ƒë·ªông ho·∫∑c h∆∞·ªõng d·∫´n download
            self.download_model(model_path)
        self.net.load_state_dict(self.copyStateDict(torch.load(model_path, map_location='cpu')))
        self.net.eval()
    
    def copyStateDict(self, state_dict):
        if list(state_dict.keys())[0].startswith("module"):
            start_idx = 1
        else:
            start_idx = 0
        new_state_dict = {}
        for k, v in state_dict.items():
            name = ".".join(k.split(".")[start_idx:])
            new_state_dict[name] = v
        return new_state_dict
    
    def detect_text_regions(self, image):
        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh
        img_resized, target_ratio, size_heatmap = self.resize_aspect_ratio(image, 1280, cv2.INTER_LINEAR, 1.5)
        ratio_h = ratio_w = 1 / target_ratio

        # Chu·∫©n h√≥a ·∫£nh
        x = self.normalizeMeanVariance(img_resized)
        x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]
        x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]

        # Forward pass
        with torch.no_grad():
            y, _ = self.net(x)

        # L·∫•y heatmaps
        score_text = y[0,:,:,0].cpu().data.numpy()
        score_link = y[0,:,:,1].cpu().data.numpy()

        # Post-processing
        boxes, polys = self.getDetBoxes(score_text, score_link, 0.7, 0.4, 0.4, False)
        boxes = self.adjustResultCoordinates(boxes, ratio_w, ratio_h)
        polys = self.adjustResultCoordinates(polys, ratio_w, ratio_h)
        
        return boxes, polys, score_text
    
class TextDetectionOCR:
    def __init__(self):
        # Kh·ªüi t·∫°o CRAFT detector
        self.craft_detector = CRAFT()
        
       # Kh·ªüi t·∫°o Transformer OCR (TrOCR)
        try:
            self.trocr_processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-printed")
            self.trocr_model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-printed")
            print("‚úÖ Transformer OCR (TrOCR) kh·ªüi t·∫°o th√†nh c√¥ng")
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói Transformer OCR: {e}")
            self.trocr_processor = None
            self.trocr_model = None
        
        print("‚úÖ H·ªá th·ªëng kh·ªüi t·∫°o th√†nh c√¥ng!")

    def recognize_with_trocr(self, image_region):
        """Nh·∫≠n d·∫°ng text v·ªõi Transformer OCR (TrOCR)"""
        if self.trocr_processor is None or self.trocr_model is None:
            return "", 0.0
        
        try:
            # Chuy·ªÉn sang PIL Image
            pil_image = Image.fromarray(image_region)
            
            # Ti·ªÅn x·ª≠ l√Ω
            pixel_values = self.trocr_processor(images=pil_image, return_tensors="pt").pixel_values
            
            # Nh·∫≠n d·∫°ng
            generated_ids = self.trocr_model.generate(pixel_values)
            text = self.trocr_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            
            return text, 0.9  # TrOCR kh√¥ng c√≥ confidence score
            
        except Exception as e:
            print(f"‚ùå Transformer OCR error: {e}")
            return "", 0.0

    def preprocess_image(self, image):
        """Ti·ªÅn x·ª≠ l√Ω ·∫£nh ƒë·ªÉ c·∫£i thi·ªán OCR"""
        try:
            # Chuy·ªÉn sang grayscale
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            
            # TƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            enhanced = clahe.apply(gray)
            
            # L√†m s·∫Øc n√©t
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(enhanced, -1, kernel)
            
            # Chuy·ªÉn l·∫°i th√†nh RGB
            result = cv2.cvtColor(sharpened, cv2.COLOR_GRAY2RGB)
            return result
            
        except Exception as e:
            print(f"L·ªói ti·ªÅn x·ª≠ l√Ω ·∫£nh: {e}")
            return image


    def load_image(self, image_path):
        """Load ·∫£nh t·ª´ file path"""
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        return image
    
    def process_image(self, image_path):
        """X·ª≠ l√Ω ·∫£nh v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ nh·∫≠n di·ªán"""
        # Load ·∫£nh
        image = self.load_image(image_path)
        if image is None:
            return None
            
        original_image = image.copy()
        
        # Ph√°t hi·ªán v√πng vƒÉn b·∫£n v·ªõi CRAFT
        print("üîç ƒêang ph√°t hi·ªán v√πng vƒÉn b·∫£n v·ªõi CRAFT...")
        boxes, polys, score_text = self.craft_detector.detect_text_regions(image)
        
        print(f"üìä T√¨m th·∫•y {len(boxes)} v√πng vƒÉn b·∫£n")
        
        # V·∫Ω bounding boxes v√† nh·∫≠n d·∫°ng vƒÉn b·∫£n
        results = []
        image_with_boxes = original_image.copy()
        
        for i, box in enumerate(boxes):
            try:
                # Chuy·ªÉn ƒë·ªïi t·ªça ƒë·ªô box
                box = box.astype(np.int32)
                
                # Crop v√πng vƒÉn b·∫£n t·ª´ ·∫£nh g·ªëc
                x_min, y_min = box[:, 0].min(), box[:, 1].min()
                x_max, y_max = box[:, 0].max(), box[:, 1].max()
                
                # T√≠nh margin
                region_width = x_max - x_min
                region_height = y_max - y_min
                margin_w = max(15, region_width // 3)
                margin_h = max(15, region_height // 3)
                
                x_min = max(0, x_min - margin_w)
                y_min = max(0, y_min - margin_h)
                x_max = min(image.shape[1], x_max + margin_w)
                y_max = min(image.shape[0], y_max + margin_h)
                
                text_region = original_image[y_min:y_max, x_min:x_max]
                
                # Nh·∫≠n d·∫°ng vƒÉn b·∫£n v·ªõi Transformer OCR
                if text_region.size > 0:
                    print(f"  üîç X·ª≠ l√Ω v√πng {i+1} - K√≠ch th∆∞·ªõc: {text_region.shape}")
                    
                    # TI·ªÄN X·ª¨ L√ù ·∫¢NH
                    processed_region = self.enhance_image_quality(text_region)
                    
                    # OCR v·ªõi Transformer OCR
                    detected_text, confidence = self.recognize_with_trocr(processed_region)
                    
                    print(f"  ‚úÖ V√πng {i+1}: '{detected_text}' (confidence: {confidence:.2f})")
                    
                    # V·∫Ω bounding box
                    cv2.polylines(image_with_boxes, [box], True, (0, 255, 0), 2)
                    
                    # Th√™m text label
                    if detected_text:
                        cv2.putText(image_with_boxes, detected_text, 
                                (box[0][0], box[0][1] - 10), 
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
                    
                    results.append({
                        'bbox': box.tolist(),
                        'text': detected_text,
                        'confidence': confidence,
                        'coordinates': {
                            'x_min': x_min,
                            'y_min': y_min,
                            'x_max': x_max,
                            'y_max': y_max
                        }
                    })
                    
            except Exception as e:
                print(f"‚ùå L·ªói x·ª≠ l√Ω v√πng {i+1}: {e}")
                continue
        
        return {
            'original_image': original_image,
            'image_with_boxes': image_with_boxes,
            'heatmap': score_text,
            'detections': results
        }

    def enhance_image_quality(self, image):
        """N√¢ng cao ch·∫•t l∆∞·ª£ng ·∫£nh cho OCR"""
        try:
            # 1. Upscale ·∫£nh nh·ªè
            h, w = image.shape[:2]
            if h * w < 5000:  # N·∫øu ·∫£nh qu√° nh·ªè
                scale_factor = 4
            elif h * w < 10000:
                scale_factor = 3
            else:
                scale_factor = 2
                
            upscaled = cv2.resize(image, (w * scale_factor, h * scale_factor), 
                                interpolation=cv2.INTER_CUBIC)
            
            # 2. TƒÉng ƒë·ªô s√°ng v√† t∆∞∆°ng ph·∫£n
            lab = cv2.cvtColor(upscaled, cv2.COLOR_RGB2LAB)
            l, a, b = cv2.split(lab)
            
            # TƒÉng ƒë·ªô s√°ng
            clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
            l_enhanced = clahe.apply(l)
            
            # K·∫øt h·ª£p l·∫°i
            lab_enhanced = cv2.merge([l_enhanced, a, b])
            brightened = cv2.cvtColor(lab_enhanced, cv2.COLOR_LAB2RGB)
            
            # 3. L√†m s·∫Øc n√©t
            kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
            sharpened = cv2.filter2D(brightened, -1, kernel)
            
            # 4. Gi·∫£m nhi·ªÖu
            denoised = cv2.medianBlur(sharpened, 3)
            
            return denoised
            
        except Exception as e:
            print(f"L·ªói n√¢ng cao ch·∫•t l∆∞·ª£ng ·∫£nh: {e}")
            return image
    
    def visualize_results(self, result, save_path=None):
        """Hi·ªÉn th·ªã k·∫øt qu·∫£"""
        fig, axes = plt.subplots(1, 2, figsize=(20, 10))
        
        # Hi·ªÉn th·ªã ·∫£nh g·ªëc
        axes[0].imshow(result['original_image'])
        axes[0].set_title('·∫¢nh g·ªëc')
        axes[0].axis('off')
        
        # Hi·ªÉn th·ªã ·∫£nh v·ªõi bounding boxes
        axes[1].imshow(result['image_with_boxes'])
        axes[1].set_title('V√πng vƒÉn b·∫£n ƒë∆∞·ª£c ph√°t hi·ªán')
        axes[1].axis('off')
        
        # Hi·ªÉn th·ªã heatmap
        plt.figure(figsize=(10, 8))
        plt.imshow(result['heatmap'], cmap='hot')
        plt.title('Heatmap t·ª´ CRAFT')
        plt.colorbar()
        plt.axis('off')
        
        if save_path:
            plt.savefig(save_path, bbox_inches='tight', dpi=300)
        
        plt.tight_layout()
        plt.show()
        
        # In k·∫øt qu·∫£ nh·∫≠n d·∫°ng
        print("\n=== K·∫æT QU·∫¢ NH·∫¨N D·∫†NG VƒÇN B·∫¢N ===")
        for i, detection in enumerate(result['detections']):
            print(f"V√πng {i+1}:")
            print(f"  Text: {detection['text']}")
            print(f"  Confidence: {detection['confidence']:.4f}")
            print(f"  Coordinates: {detection['coordinates']}")
            print("-" * 50)